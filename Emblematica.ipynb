{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import csv for book URLs and get all titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# read the csv file for book urls\n",
    "title_all = pd.read_csv(\"BooksURLs.csv\")\n",
    "title_add= list(title_all.newUrl)\n",
    "len(title_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all VIAF/DNB links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import Lib.urllib.request\n",
    "import rdflib\n",
    "from itertools import islice\n",
    "import requests\n",
    "import glob\n",
    "\n",
    "from rdflib import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the name VIAF/DNB urls\n",
    "viaf_unique = []\n",
    "names_has_viaf = []\n",
    "names_no_viaf = []\n",
    "viaf_else = []\n",
    "n = 0\n",
    "d = {}\n",
    "for i in title_add:\n",
    "    d[i] = {}\n",
    "    page = requests.get(i)\n",
    "    soup = BeautifulSoup(page.content,'xml')\n",
    "    creators = soup.find_all('name')\n",
    "    for m in creators:\n",
    "        if 'valueURI' in m:\n",
    "            names_has_viaf.append(m.namePart.text)\n",
    "        else:\n",
    "            try:\n",
    "                if m['valueURI']:\n",
    "                    names_has_viaf.append(m.namePart.text)\n",
    "            except:\n",
    "                if 'type' in m:\n",
    "                    if m['type'] == \"personal\":\n",
    "                        names_no_viaf.append(m.namePart.text)\n",
    "                else:\n",
    "                    n+=1\n",
    "                    try:\n",
    "                        viaf_else.append(m.namePart.text)\n",
    "                    except:\n",
    "                        viaf_else.append(m)\n",
    "        try:\n",
    "            d[i][str(m['valueURI'].rstrip(\"/\"))] = {}\n",
    "            if str(m['valueURI'].rstrip(\"/\")) not in viaf_unique:\n",
    "                viaf_unique.append(m['valueURI'].rstrip(\"/\"))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of names with viaf urls and without viaf urls\n",
    "print(len(names_has_viaf))\n",
    "print(len(set(names_no_viaf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of names with unique viaf/dnb links\n",
    "all_names = []\n",
    "names = []\n",
    "for link in d.keys():\n",
    "    for viaf in d[link]:\n",
    "        if viaf not in names:\n",
    "            all_names.append(d[link][viaf])\n",
    "            names.append(viaf)\n",
    "        else:\n",
    "            pass\n",
    "print(len(all_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of names with unique dnb links\n",
    "dnb_viaf = []\n",
    "n = 0\n",
    "for i in viaf_unique:\n",
    "    if not i.startswith(\"http://viaf.org/viaf/\") and i not in dnb_viaf:\n",
    "        dnb_viaf.append(i)\n",
    "    else:\n",
    "        n+=1\n",
    "print(len(dnb_viaf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of names with unique viaf links\n",
    "origin_viaf = []\n",
    "for i in viaf_unique:\n",
    "    if i.startswith(\"http://viaf.org/viaf/\") and i not in dnb_viaf:\n",
    "        origin_viaf.append(i.rstrip(\"/\"))\n",
    "print(len(origin_viaf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covert DNB IDs to VIAF IDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cover DNB IDs to VIAF IDs using VIAF Translate Source API\n",
    "new_viaf = {}\n",
    "failed_covert_dnb_viaf = []\n",
    "n = 0\n",
    "for link in dnb_viaf:\n",
    "    dnb_id = link.strip(\"http://d-nb.info/gnd/\")\n",
    "    dnb_html = \"http://www.viaf.org/viaf/sourceID/DNB%7c\" + dnb_id\n",
    "    r = requests.get(dnb_html)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    print(n)\n",
    "    n += 1\n",
    "    try:\n",
    "        viaf_id = soup.findAll(\"title\")[0].text\n",
    "        viaf_link = \"http://viaf.org/viaf/\" + viaf_id\n",
    "        if viaf_id != \"Document not found\":\n",
    "            if link not in new_viaf:\n",
    "                new_viaf[link] = viaf_link\n",
    "            if link in new_viaf and viaf_link != new_viaf[link]:\n",
    "                print(new_viaf[link], viaf_link)\n",
    "        else:\n",
    "            failed_covert_dnb_viaf.append(link)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of duplicated viaf IDs after conversion\n",
    "n = 0\n",
    "for v in new_viaf.values():\n",
    "    if v in origin_viaf:\n",
    "        n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of DNB IDs that cannot be mapped to VIAF\n",
    "all_link_after_covert = []\n",
    "d1 = {}\n",
    "for key in d:\n",
    "    d1[key] = {}\n",
    "    for kk in d[key]:\n",
    "        if kk not in all_link_after_covert:\n",
    "            all_link_after_covert.append(kk)\n",
    "        if kk in new_viaf:\n",
    "            d1[key][new_viaf[kk]] = {}\n",
    "        else:\n",
    "            kk = kk.rstrip(\"/\")\n",
    "            d1[key][kk] = {}\n",
    "            \n",
    "n = 0\n",
    "no_viaf_dnb_unique = []\n",
    "for k in d1:\n",
    "    for kk in d1[k]:\n",
    "        if not kk.startswith(\"http://viaf.org/viaf/\") and kk not in no_viaf_dnb_unique:\n",
    "            n+=1\n",
    "            no_viaf_dnb_unique.append(kk)\n",
    "\n",
    "len(no_viaf_dnb_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find all name authorities + remove viaf IDs that have no name authorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = {}\n",
    "failed_links = [] #cannot open viaf links\n",
    "for link in d1.keys():\n",
    "    d2[link]={}\n",
    "    for viaf in d1[link].keys():\n",
    "        d2[link][viaf] = {}\n",
    "        if viaf.startswith(\"http://viaf.org/viaf/\"):\n",
    "            l = viaf.rstrip(\"/\") + \"/justlinks.json\" \n",
    "            r = requests.get(l)\n",
    "            try:\n",
    "                json = r.json()\n",
    "                if \"WKP\" in json and json[\"WKP\"] != []:\n",
    "                    wiki = json[\"WKP\"]\n",
    "                    d2[link][viaf][\"wiki\"] = {}\n",
    "                    d2[link][viaf][\"wiki\"][wiki[0]]=[]\n",
    "                if \"BNF\" in json and json[\"BNF\"] != []:\n",
    "                    bnf = json[\"BNF\"]\n",
    "                    d2[link][viaf][\"bnf\"]= {}\n",
    "                    d2[link][viaf][\"bnf\"][bnf[0]]=[]\n",
    "                if \"DNB\" in json and json[\"DNB\"]!=[]:\n",
    "                    dnb = json[\"DNB\"]\n",
    "                    d2[link][viaf][\"dnb\"] = {}\n",
    "                    d2[link][viaf][\"dnb\"][dnb[0]] = []\n",
    "                if \"LC\" in json and json[\"LC\"] !=[]:\n",
    "                    lc = json[\"LC\"]\n",
    "                    d2[link][viaf][\"lc\"] = {}\n",
    "                    if lc[0] != \"LNB:BSLC;=BA\":\n",
    "                        d2[link][viaf][\"lc\"][lc[0]] = []\n",
    "                    else:\n",
    "                        print(lc)\n",
    "                        d2[link][viaf][\"lc\"][lc[1]] = []\n",
    "                if \"JPG\" in json and json[\"JPG\"]!=[]:\n",
    "                    getty = json[\"JPG\"]\n",
    "                    d2[link][viaf][\"getty\"] = {}\n",
    "                    d2[link][viaf][\"getty\"][getty[0]] = []\n",
    "            except:\n",
    "                if (link,viaf) not in failed_links:\n",
    "                    failed_links.append((link, viaf))\n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            d2[link][viaf]={}\n",
    "            d2[link][viaf][\"dnb\"]= {}\n",
    "            d2[link][viaf][\"dnb\"][viaf] = []\n",
    "            print(d2[link][viaf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of names that exits in at least one of the selected authorities\n",
    "all_names2 = []\n",
    "names2 = []\n",
    "\n",
    "for link in d2.keys():\n",
    "    for viaf in d2[link]:\n",
    "        if viaf not in names2:\n",
    "            all_names2.append(d2[link][viaf])\n",
    "            names2.append(viaf)\n",
    "        else:\n",
    "            pass\n",
    "print(len(all_names2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the dictionary to a csv file\n",
    "with open(\"results1.json\", \"r\", encoding='utf-8') as read_file:\n",
    "    d2 = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of obsolete VIAF IDs\n",
    "varify_failed = []\n",
    "for link in d2:\n",
    "    for viaf in d2[link]:\n",
    "        if d2[link][viaf] == {}:\n",
    "            if (link, viaf) not in failed_links:\n",
    "                print((link, viaf))\n",
    "            if (link,viaf) not in varify_failed:\n",
    "                varify_failed.append((link,viaf))\n",
    "no_viaf = [i[1] for i in varify_failed if i not in no_viaf]\n",
    "no_viaf = list(set(no_viaf))\n",
    "print(len(no_viaf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually find the IDs of selected authorities for names that are available in DNB but not in VIAF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count authorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results2.json\", \"r\", encoding='utf-8') as read_file:\n",
    "    d4 = json.load(read_file)\n",
    "d = d4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of person name entities that are available in the selected authorities\n",
    "c_dnb = []\n",
    "dnb_unique = []\n",
    "c_wiki = []\n",
    "wiki_unique = []\n",
    "c_getty = []\n",
    "getty_unique = []\n",
    "c_lc = []\n",
    "lc_unique = []\n",
    "c_bnf = []\n",
    "bnf_unique = []\n",
    "for title in d:\n",
    "    for viaf in d[title]:\n",
    "        for aut in d[title][viaf]:\n",
    "            if aut == \"wiki\" and viaf not in c_wiki:\n",
    "                c_wiki.append(viaf)\n",
    "                wiki_unique.append(list(d[title][viaf][\"wiki\"].keys())[0])\n",
    "            if aut == \"dnb\" and viaf not in c_dnb:\n",
    "                c_dnb.append(viaf)\n",
    "                dnb_unique.append(list(d[title][viaf][\"dnb\"].keys())[0])\n",
    "            if aut == \"bnf\" and viaf not in c_bnf:\n",
    "                c_bnf.append(viaf)\n",
    "                bnf_unique.append(list(d[title][viaf][\"bnf\"].keys())[0])\n",
    "            if aut == \"lc\" and viaf not in c_lc:\n",
    "                c_lc.append(viaf)\n",
    "                lc_unique.append(list(d[title][viaf][\"lc\"].keys())[0])\n",
    "            if aut == \"getty\" and viaf not in c_getty:\n",
    "                c_getty.append(viaf)\n",
    "                getty_unique.append(list(d[title][viaf][\"getty\"].keys())[0])\n",
    "                \n",
    "print(\"wiki \", len(c_wiki))\n",
    "print(\"getty \",len(c_getty))\n",
    "print(\"lc \", len(c_lc))\n",
    "print(\"bnf \", len(c_bnf))\n",
    "print(\"dnb \", len(c_dnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get property names for name entities in DNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_dnb = {}\n",
    "error = []\n",
    "not_processed = []\n",
    "n = 0\n",
    "for dnb_id in dnb_unique:\n",
    "    print(n)\n",
    "    n += 1\n",
    "    renewed_url = 'https://hub.culturegraph.org/entityfacts/' + dnb_id.lstrip(\"https://d-nb.info/gnd/\")\n",
    "    print(renewed_url)\n",
    "    try:\n",
    "        page = requests.get(renewed_url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        newDictionary=json.loads(str(soup))\n",
    "        r = list(newDictionary.keys())\n",
    "        print(r)\n",
    "        if r != ['Error']:\n",
    "            d_dnb[dnb_id] = r\n",
    "        else:\n",
    "            error.append(dnb_id)\n",
    "    except:\n",
    "        not_processed.append(dnb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame for properties\n",
    "no_dnb = []\n",
    "d1 = {}\n",
    "for link in d:\n",
    "    d1[link] = {}\n",
    "    for viaf in d[link]:\n",
    "        d1[link][viaf] = {}\n",
    "        for k in d[link][viaf]:\n",
    "            if k == \"dnb\":\n",
    "                key = list(d[link][viaf][k].keys())[0]\n",
    "                if key in d_dnb:\n",
    "                    d1[link][viaf][\"dnb\"] = {}\n",
    "                    d1[link][viaf][\"dnb\"][key] = d_dnb[key]\n",
    "                else:\n",
    "                    print((link,viaf,key))\n",
    "                    no_dnb.append((link,viaf,key))\n",
    "            if k ==\"lc\":\n",
    "                key = list(d[link][viaf][k].keys())[0]\n",
    "                d1[link][viaf][\"lc\"] = {}\n",
    "                d1[link][viaf][\"lc\"][key] = []\n",
    "            if k ==\"getty\":\n",
    "                key = list(d[link][viaf][k].keys())[0]\n",
    "                d1[link][viaf][\"getty\"] = {}\n",
    "                d1[link][viaf][\"getty\"][key] = []\n",
    "            if k ==\"bnf\":\n",
    "                key = list(d[link][viaf][k].keys())[0]\n",
    "                d1[link][viaf][\"bnf\"] = {}\n",
    "                d1[link][viaf][\"bnf\"][key] = []\n",
    "            if k ==\"wiki\":\n",
    "                key = list(d[link][viaf][k].keys())[0]\n",
    "                d1[link][viaf][\"wiki\"] = {}\n",
    "                d1[link][viaf][\"wiki\"][key] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get property names for name entities in wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_id = [\"Repertorium van ambtsdragers en ambtenaren id\",'PSS-Archi architect id',  'EAGLE id',  'BALaT person/organisation id',  'NMVW id',  'National Library of Korea Identifier',  'Swedish Open Cultural Heritage URI', \"Libris-URI\", \"J. Paul Getty Museum artist id\", \"Royal Academy new identifier\", \"Image Archive, Herder Institute\", \"Europeana entity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_wiki = {}    \n",
    "n = 0\n",
    "cannot = []\n",
    "for key in wiki_unique:\n",
    "    w_link = w_link = '<http://www.wikidata.org/entity/' + key +\">\"\n",
    "    try:\n",
    "        sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "        # From https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries/examples#Cats\n",
    "        sparql.setQuery(\"\"\"\n",
    "        SELECT DISTINCT ?p ?pLabel ?propLabel\n",
    "        WHERE \n",
    "        {\n",
    "           %s  ?p ?o.\n",
    "\n",
    "          SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". } \n",
    "          ?prop wikibase:directClaim ?p .\n",
    "        }\n",
    "\n",
    "        \"\"\"%w_link)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        results = sparql.query().convert()\n",
    "        results_df = pd.io.json.json_normalize(results['results']['bindings'])\n",
    "        r = [i for i in list(results_df[\"propLabel.value\"]) if \"ID\" not in i]\n",
    "        d_wiki[key] = r\n",
    "    except:\n",
    "        cannot.append(key)\n",
    "    print(n)\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in d1:\n",
    "    for viaf in d1[link]:\n",
    "        for k in d1[link][viaf]:\n",
    "            if k == \"wiki\":\n",
    "                wiki_id = list(d1[link][viaf][k].keys())[0]\n",
    "                d1[link][viaf][k][wiki_id] = new_wiki_d[wiki_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get property names for name entities in BNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "d_bnf = {}\n",
    "bnf_problem = []\n",
    "n1= 0\n",
    "for key in bnf_unique:\n",
    "    if key[:3] == 'FRB':\n",
    "        bnf_link = \"https://data.bnf.fr/en/\" + key[5:-1]\n",
    "    else:\n",
    "        bnf_link = \"https://data.bnf.fr/en/\" + key[-9:-1]\n",
    "    try:\n",
    "        page = requests.get(bnf_link)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        n = 0\n",
    "        r = []\n",
    "        for row in soup.find_all('td'):\n",
    "            if n % 2 == 0:\n",
    "                r.append(row.text.strip().strip(\"\\xa0:\"))\n",
    "            n += 1\n",
    "        r2 = []\n",
    "        h3 = soup.find_all(\"h3\")\n",
    "        for i in h3:\n",
    "            r2.append(re.sub(r'\\([^)]*\\)', '', i.text.strip(\"\\xa0:\").strip().replace(\" \",\"\").replace(\"\\n\",\"\")))\n",
    "        r += r2\n",
    "        print(r)\n",
    "        d_bnf[key] = r\n",
    "    except:\n",
    "        bnf_problem.append(key)\n",
    "    n1+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in d1:\n",
    "    for viaf in d1[link]:\n",
    "        for k in d1[link][viaf]:\n",
    "            if k == \"bnf\":\n",
    "                bnf_id = list(d1[link][viaf][k].keys())[0]\n",
    "                d1[link][viaf][k][bnf_id] = d_bnf[bnf_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get property names for name entities in Getty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic = ['Note',\n",
    " 'Names',\n",
    " 'preferred',\n",
    " 'Nationalities',\n",
    " 'preferred',\n",
    " 'Roles',\n",
    " 'preferred',\n",
    " 'Gender',\n",
    " 'Birth and Death Places',\n",
    " 'List/Hierarchical Position',\n",
    " 'Related People or Corporate Bodies',\n",
    " 'Biographies',\n",
    " 'Additional Names',\n",
    " 'Sources and Contributors',\n",
    " 'Subject',\n",
    "'Events', 'Note']\n",
    "d_getty = {}\n",
    "for key in getty_unique:\n",
    "    print(\"For Getty\", key)\n",
    "    g_link = 'http://vocab.getty.edu/page/ulan/' + key\n",
    "    page = requests.get(g_link)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    a = soup.find_all(\"span\", {\"class\": \"page\"})\n",
    "    r = []\n",
    "    for i in a:\n",
    "        b = i.find_all(\"b\")\n",
    "        for m in b:\n",
    "            r.append(m.text.strip().strip(':'))   \n",
    "    dump = list(set(r) - set(basic))\n",
    "    r = [i for i in r if i not in dump]\n",
    "    r1 = []\n",
    "    for i in range(len(r)):\n",
    "        if i > 0:\n",
    "            if r[i-1] == \"Names\" and r[i] == 'preferred':\n",
    "                r1.append(\"Names preferred\")\n",
    "            elif r[i-1] == \"Nationalities\" and r[i] == 'preferred':\n",
    "                r1.append(\"Nationalities preferred\") \n",
    "            elif r[i-1] == \"Roles\" and r[i] == 'preferred':\n",
    "                r1.append(\"Roles preferred\") \n",
    "            else:\n",
    "                r1.append(r[i])\n",
    "        else:\n",
    "            r1.append(r[i])\n",
    "    d_getty[key]= r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in d1:\n",
    "    for viaf in d1[link]:\n",
    "        for k in d1[link][viaf]:\n",
    "            if k == \"getty\":\n",
    "                g_id = list(d1[link][viaf][k].keys())[0]\n",
    "                d1[link][viaf][k][g_id] = d_getty[g_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get property names for name entities in LC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_lc = {}\n",
    "for key in lc_unique:\n",
    "    lc_link1 = \"http://id.loc.gov/authorities/names/\" + key\n",
    "    page = requests.get(lc_link1)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    r = soup.findAll(\"h3\")\n",
    "    r1 = soup.findAll(\"dt\", {\"title\": \"bf:Title\"})\n",
    "    r += r1\n",
    "    r2 = []\n",
    "    for node in r:\n",
    "        r2.append(''.join(node.findAll(text=True)))\n",
    "    print(r2)\n",
    "    d_lc[key] = r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in d1:\n",
    "    for viaf in d1[link]:\n",
    "        for k in d1[link][viaf]:\n",
    "            if k == \"lc\":\n",
    "                lc_id = list(d1[link][viaf][k].keys())[0]\n",
    "                d1[link][viaf][k][lc_id] = d_lc[lc_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the data frame to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(columns=[\"book_ind\",\"link\", \"viaf\", \"dnb\",\"lc\", \"wiki\",\"bnf\",\"getty\"])\n",
    "d = d1.copy()\n",
    "n = 0\n",
    "for link in d:\n",
    "    n+=1\n",
    "    for viaf in d[link]:\n",
    "        l = [link, viaf]\n",
    "        if \"wiki\" in d[link][viaf]:\n",
    "            key = list(d[link][viaf]['wiki'].keys())[0]\n",
    "            l.append(d[link][viaf]['wiki'][key])\n",
    "        else:\n",
    "            l.append(\"\")\n",
    "        if \"dnb\" in d[link][viaf]:\n",
    "            key = list(d[link][viaf]['dnb'].keys())[0]\n",
    "            l.append(d[link][viaf]['dnb'][key])\n",
    "        else:\n",
    "            l.append(\"\")\n",
    "        if \"bnf\" in d[link][viaf]:\n",
    "            key = list(d[link][viaf]['bnf'].keys())[0]\n",
    "            l.append(d[link][viaf]['bnf'][key])\n",
    "        else:\n",
    "            l.append(\"\")\n",
    "        if \"getty\" in d[link][viaf]:\n",
    "            key = list(d[link][viaf]['getty'].keys())[0]\n",
    "            l.append(d[link][viaf]['getty'][key])\n",
    "        else:\n",
    "            l.append(\"\")\n",
    "        if \"lc\" in d[link][viaf]:\n",
    "            key = list(d[link][viaf]['lc'].keys())[0]\n",
    "            l.append(d[link][viaf]['lc'][key])\n",
    "        else:\n",
    "            l.append(\"\")\n",
    "        df1 = df1.append({\"book_ind\": n, \"link\": link, \"viaf\": viaf,\"wiki\": l[2],\"dnb\": l[3],\"bnf\": l[4],\"getty\": l[5],\"lc\":l[6]}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"results3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the number of properties for name entities in DNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnb_v1 = df1['dnb']\n",
    "dnb_unique1 = []\n",
    "for i in dnb_v1:\n",
    "    for m in i:\n",
    "        if m not in dnb_unique1:\n",
    "            dnb_unique1.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_dnb1 = {}\n",
    "for i in dnb_unique1:\n",
    "    d_dnb1[i] = 0\n",
    "count = 0\n",
    "for name in all_names:\n",
    "    if \"dnb\" in name:\n",
    "        count+=1\n",
    "        d_d1 = name['dnb']\n",
    "        dnb_key1 = list(d_d1.keys())[0]\n",
    "        dnb_list1 = d_d1[dnb_key1]\n",
    "        print(dnb_list1)\n",
    "        for i in dnb_list1:\n",
    "            d_dnb1[i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the number of properties for name entities in LC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_v1 = df1['lc']\n",
    "lc_unique1 = []\n",
    "for i in lc_v1:\n",
    "    for m in i:\n",
    "        if m not in lc_unique1:\n",
    "            lc_unique1.append(m)\n",
    "d_lc1 = {}\n",
    "for i in lc_unique1:\n",
    "    d_lc1[i] = 0\n",
    "count = 0\n",
    "for name in all_names:\n",
    "    if \"lc\" in name:\n",
    "        count+=1\n",
    "        d_d1 = name['lc']\n",
    "        lc_key1 = list(d_d1.keys())[0]\n",
    "        lc_list1 = d_d1[lc_key1]\n",
    "        print(lc_list1)\n",
    "        for i in lc_list1:\n",
    "            d_lc1[i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the number of properties for name entities in wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_v1 = df1['wiki']\n",
    "wiki_unique1 = []\n",
    "for i in wiki_v1:\n",
    "    for m in i:\n",
    "        if m not in wiki_unique1:\n",
    "            wiki_unique1.append(m)\n",
    "d_wiki1 = {}\n",
    "for i in wiki_unique1:\n",
    "    d_wiki1[i] = 0\n",
    "count = 0\n",
    "for name in all_names:\n",
    "    if \"wiki\" in name:\n",
    "        count+=1\n",
    "        d_d1 = name['wiki']\n",
    "        wiki_key1 = list(d_d1.keys())[0]\n",
    "        wiki_list1 = d_d1[wiki_key1]\n",
    "        print(wiki_list1)\n",
    "        for i in wiki_list1:\n",
    "            d_wiki1[i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the number of properties for name entities in BNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnf_v1 = df1['bnf']\n",
    "bnf_unique1 = []\n",
    "for i in bnf_v1:\n",
    "    for m in i:\n",
    "        if m not in bnf_unique1:\n",
    "            bnf_unique1.append(m)\n",
    "d_bnf1 = {}\n",
    "for i in bnf_unique1:\n",
    "    d_bnf1[i] = 0\n",
    "count = 0\n",
    "for name in all_names:\n",
    "    if \"bnf\" in name:\n",
    "        count+=1\n",
    "        d_d1 = name['bnf']\n",
    "        bnf_key1 = list(d_d1.keys())[0]\n",
    "        bnf_list1 = d_d1[bnf_key1]\n",
    "        print(bnf_list1)\n",
    "        for i in bnf_list1:\n",
    "            d_bnf1[i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the number of properties for name entities in Getty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getty_v1 = df1['getty']\n",
    "getty_unique1 = []\n",
    "for i in getty_v1:\n",
    "    for m in i:\n",
    "        if m not in getty_unique1:\n",
    "            getty_unique1.append(m)\n",
    "d_getty1 = {}\n",
    "for i in getty_unique1:\n",
    "    d_getty1[i] = 0\n",
    "count = 0\n",
    "for name in all_names:\n",
    "    if \"getty\" in name:\n",
    "        count+=1\n",
    "        d_d1 = name['getty']\n",
    "        getty_key1 = list(d_d1.keys())[0]\n",
    "        getty_list1 = d_d1[getty_key1]\n",
    "        print(getty_list1)\n",
    "        for i in getty_list1:\n",
    "            d_getty1[i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare BnF JSON with JSONLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bnf = []\n",
    "for key in bnf_unique:\n",
    "    if key[:3] == 'FRB':\n",
    "        bnf_link = \"https://data.bnf.fr/en/\" + key[5:-1]\n",
    "    else:\n",
    "        bnf_link = \"https://data.bnf.fr/en/\" + key[-9:-1]\n",
    "    all_bnf.append(bnf_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_jsonld = []\n",
    "all_json = []\n",
    "for i in all_bnf:\n",
    "    page = requests.get(i)\n",
    "    prefix = page.url\n",
    "    ld = prefix + \"rdf.jsonld\"\n",
    "    js = prefix.rstrip(\"/\") + \".json\"\n",
    "    all_jsonld.append(ld)\n",
    "    all_json.append(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jsonld\n",
    "def parse_jsonld_1(mydata, d):\n",
    "    for child in mydata['@graph']:\n",
    "        if \"dcterms:creator\" in child:\n",
    "            if \"works\" not in d:\n",
    "                d['works'] = [child[\"@id\"].replace(\"#about\", \"\")]\n",
    "            else:\n",
    "                d['works'].append(child[\"@id\"].replace(\"#about\", \"\"))\n",
    "            \n",
    "        if '#about' in child[\"@id\"]:\n",
    "            aboutauthor = child\n",
    "\n",
    "            # gender\n",
    "            if 'foaf:gender' in aboutauthor:\n",
    "                d[\"gender\"] = 'foaf:gender'\n",
    "\n",
    "            # image\n",
    "            if 'foaf:depiction' in aboutauthor:\n",
    "                d[\"image\"] = 'foaf:depiction'\n",
    "\n",
    "            # birth_date\n",
    "            for bd in birth_date_pool:\n",
    "                if bd in aboutauthor:\n",
    "                    if 'birthdate' not in d:\n",
    "                        d[\"birthdate\"] = [bd]\n",
    "                    else:\n",
    "                        d[\"birthdate\"].append(bd)\n",
    "\n",
    "            # death_date\n",
    "            for dd in death_date_pool:\n",
    "                if dd in aboutauthor:\n",
    "                    if 'deathdate' not in d:\n",
    "                        d[\"deathdate\"] = [dd]\n",
    "                    else:\n",
    "                        d[\"deathdate\"].append(dd)\n",
    "\n",
    "            # birth_place\n",
    "            for bp in birth_place_pool:\n",
    "                if bp in aboutauthor:\n",
    "                    if 'birthplace' not in d:\n",
    "                        d[\"birthplace\"] = [bp]\n",
    "                    else:\n",
    "                        d[\"birthplace\"].append(bp)\n",
    "\n",
    "            # death_place\n",
    "            for dp in death_place_pool:\n",
    "                if dp in aboutauthor:\n",
    "                    if 'deathplace' not in d:\n",
    "                        d[\"deathplace\"] = [dp]\n",
    "                    else:\n",
    "                        d[\"deathplace\"].append(dp)\n",
    "\n",
    "             # language\n",
    "            for lan in language_pool:\n",
    "                if lan in aboutauthor:\n",
    "                    if 'language' not in d:\n",
    "                        d[\"language\"] = [lan]\n",
    "                    else:\n",
    "                        d[\"language\"].append(lan)\n",
    "\n",
    "            # biographical\n",
    "            for bio in biographicalInformation_pool:\n",
    "                if bio in aboutauthor:\n",
    "                    if 'notes' not in d:\n",
    "                        d['notes'] = [bio]\n",
    "                    else:\n",
    "                        d['notes'].append(bio) \n",
    "            \n",
    "            # nationality\n",
    "            for nation in nationality_pool:\n",
    "                if nation in aboutauthor:\n",
    "                    if 'nationality' not in d:\n",
    "                        d[\"nationality\"] = [nation]\n",
    "                    else:\n",
    "                        d[\"nationality\"\n",
    "                         ].append(nation)\n",
    "            \n",
    "\n",
    "        if \"#\" not in child[\"@id\"]:\n",
    "            generalauthor = child\n",
    "            # variant names\n",
    "            if 'skos:altLabel' in generalauthor:\n",
    "                if \"variant_names\" not in d:\n",
    "                    d[\"variant_names\"] = \"skos:altLabel\"\n",
    "                    print(\"variant name\")\n",
    "\n",
    "            # biographical\n",
    "            if 'skos:note' in generalauthor:\n",
    "                if 'notes' not in d:\n",
    "                    d['notes'] = [\"skos:note\"]\n",
    "                else:\n",
    "                    d['notes'].append(\"skos:note\") \n",
    "\n",
    "                print(\"note\")\n",
    "\n",
    "        if \"#Expression\" in child[\"@id\"]:\n",
    "            if \"contributors\" not in d: \n",
    "                d[\"contributors\"] = [child]\n",
    "            else:\n",
    "                d[\"contributors\"].append(child)\n",
    "    return d\n",
    "\n",
    "_int = 0\n",
    "d_author = {}\n",
    "for uri in all_jsonld:\n",
    "    print(_int)\n",
    "    key = uri.replace('/rdf.jsonld', \"\")\n",
    "    # The content type you want to set in the Request Headers.\n",
    "    # This example is for RDF/XML\n",
    "    request_headers = {'Accept': 'application/rdf+xml'}\n",
    "    # Build the request with the URI and Header parameters \n",
    "    request = urllib2.Request(uri, headers = request_headers)\n",
    "    # Fetch the request\n",
    "    response = urllib2.urlopen(request)\n",
    "    # Read and Print the request\n",
    "    data1 = response.read()\n",
    "    dict_str = data1.decode(\"UTF-8\")\n",
    "    mydata = ast.literal_eval(dict_str)\n",
    "    d = {}\n",
    "    d = parse_jsonld_1(mydata,d)\n",
    "    d_author[key] = d\n",
    "    _int += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json\n",
    "unwanted = ['name','firstname','site_name','type','isni','ark','modification-date','url','surname','stopdate','startdate','notice-type','title','label']\n",
    "c_notes= 0\n",
    "c_works = 0\n",
    "d_json = {}\n",
    "for i in all_json:\n",
    "    r = requests.get(i)\n",
    "    js = r.json()\n",
    "    js_keys = list(js[0].keys())\n",
    "    want = list(set(js_keys)-set(unwanted))\n",
    "    if len(js[0]['works']) == 0:\n",
    "        want.remove(\"works\")\n",
    "        c_works += 1\n",
    "    if len(js[0]['notes']) == 0:\n",
    "        want.remove(\"notes\")\n",
    "        c_notes+=1\n",
    "    j_key = i.replace('.json', \"\")\n",
    "    d_json[j_key] = want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare json with jsonld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "for key in (d_json.keys() | d_jsonld.keys()):\n",
    "    if key in d_json: result.setdefault(key, []).append(d_json[key])\n",
    "    if key in d_jsonld: result.setdefault(key, []).append(d_jsonld[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(result, orient='index', columns=['JSON', 'JSONLD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare keys\n",
    "jsonld_image = []\n",
    "jsonld_notes = []\n",
    "json_birthdate = []\n",
    "json_deathdate = []\n",
    "category = []\n",
    "for k in d_error.keys():\n",
    "    for pair in d_error[k]:\n",
    "        if pair == ('jsonld_does_not_have', 'image'):\n",
    "            jsonld_image.append(k)\n",
    "            \n",
    "        elif pair == ('jsonld_does_not_have', 'notes'):\n",
    "            jsonld_notes.append(k)\n",
    "        \n",
    "        elif pair == ('json_does_not_have', 'birthdate'):\n",
    "            json_birthdate.append(k)\n",
    "            \n",
    "        else:\n",
    "            json_deathdate.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the value of roles\n",
    "all_roles = []\n",
    "for url in d_jsonld:\n",
    "    if 'works' in d_jsonld[url]:\n",
    "        for i in d_jsonld[url]['works']:\n",
    "            for k in i:\n",
    "                if 'bnfroles' in k:\n",
    "                    m = 'https://data.bnf.fr/en/vocabulary/roles/' + k.split(\":\")[1]\n",
    "                    if m not in all_roles:\n",
    "                        all_roles.append(m)\n",
    "dic_roles = {}\n",
    "for url in all_roles:\n",
    "    if url not in dic_roles:\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "        role_string = soup.findAll(\"title\")[0].text\n",
    "        dic_roles[url] = role_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of properties\n",
    "birth_date_pool = ['bio:birth','rdaa:P50121','rdagroup2elements:dateOfBirth','bnf-onto:firstYear']\n",
    "death_date_pool = ['bio:death','rdaa:P50120', 'rdagroup2elements:dateOfDeath', 'bnf-onto:lastYear']\n",
    "birth_place_pool = ['rdagroup2elements:placeOfBirth', 'rdaa:P50119']\n",
    "death_place_pool = ['rdagroup2elements:placeOfDeath', 'rdaa:P50118']\n",
    "language_pool = ['rdagroup2elements:languageOfThePerson','rdaa:P50102' ]\n",
    "biographicalInformation_pool = ['rdaa:P50113', 'rdagroup2elements:biographicalInformation']\n",
    "nationality_pool = [\"rdaa:P50097\", \"rdagroup2elements:countryAssociatedWithThePerson\"]\n",
    "\n",
    "d_birthdate = {'bio:birth': 0,'rdaa:P50121': 0,'rdagroup2elements:dateOfBirth':0,'bnf-onto:firstYear':0}\n",
    "d_deathdate = {'bio:death':0 ,'rdaa:P50120':0, 'rdagroup2elements:dateOfDeath':0, 'bnf-onto:lastYear':0}\n",
    "d_birthplace = {'rdagroup2elements:placeOfBirth' : 0, 'rdaa:P50119':0}\n",
    "d_deathplace = {'rdagroup2elements:placeOfDeath':0, 'rdaa:P50118':0}\n",
    "d_language= {'rdagroup2elements:languageOfThePerson': 0,'rdaa:P50102':0}   \n",
    "d_bio = {'rdaa:P50113':0, 'rdagroup2elements:biographicalInformation':0, \"skos:note\":0}\n",
    "d_nation = {\"rdaa:P50097\":0, \"rdagroup2elements:countryAssociatedWithThePerson\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in d_jsonld.values():\n",
    "    if \"notes\" in v:\n",
    "        for a in v[\"notes\"]:\n",
    "            d_bio[a] += 1\n",
    "    if \"birthdate\" in v:\n",
    "        for b in v[\"birthdate\"]:\n",
    "            d_birthdate[b] += 1\n",
    "    if \"deathdate\" in v:\n",
    "        for c in v[\"deathdate\"]:\n",
    "            d_deathdate[c] += 1\n",
    "    if \"birthplace\" in v:\n",
    "        for e in v[\"birthplace\"]:\n",
    "            d_birthplace[e] += 1\n",
    "    if \"deathplace\" in v:\n",
    "        for f in v[\"deathplace\"]:\n",
    "            d_deathplace[f] += 1\n",
    "    if \"language\" in v:\n",
    "        for d in v[\"language\"]:\n",
    "            d_language[d] += 1\n",
    "    if \"nationality\" in v:\n",
    "        for g in v[\"nationality\"]:\n",
    "            d_nation[g] += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no = []\n",
    "for i in d_json:\n",
    "    r = requests.get(i+\".json\")\n",
    "    js = r.json()\n",
    "    if 'notes' in js[0]:\n",
    "        no.append((i,js[0][\"notes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_bd = 0\n",
    "c_dd = 0\n",
    "for i in d_json:\n",
    "    if \"birthdate\" in d_json[i]:\n",
    "        c_bd +=1\n",
    "    if \"deathdate\" in d_json[i]:\n",
    "        c_dd +=1\n",
    "c_bd1 = 0\n",
    "c_dd1 = 0\n",
    "for i in d_jsonld:\n",
    "    if \"birthdate\" in d_jsonld[i]:\n",
    "        c_bd1 +=1\n",
    "    if \"deathdate\" in d_jsonld[i]:\n",
    "        c_dd1 +=1\n",
    "\n",
    "c_vn = 0\n",
    "c_ge = 0\n",
    "c_na = 0\n",
    "for i in d_jsonld:\n",
    "    if \"variant_names\" in d_jsonld[i]:\n",
    "        c_vn +=1\n",
    "    if \"gender\" in d_jsonld[i]:\n",
    "        c_ge +=1\n",
    "    if \"nationality\" in d_jsonld[i]:\n",
    "        c_na +=1\n",
    "c_bp = 0\n",
    "c_dp = 0\n",
    "for i in d_jsonld:\n",
    "    if \"birthplace\" in d_jsonld[i]:\n",
    "        c_bp +=1\n",
    "    if \"deathplace\" in d_jsonld[i]:\n",
    "        c_dp +=1\n",
    "\n",
    "c_lan = 0\n",
    "for i in d_jsonld:\n",
    "    if \"language\" in d_jsonld[i]:\n",
    "        c_lan +=1\n",
    "\n",
    "c_ima = 0\n",
    "for i in d_jsonld:\n",
    "    if \"image\" in d_jsonld[i]:\n",
    "        c_ima +=1\n",
    "c_no = 0\n",
    "for i in d_jsonld:\n",
    "    if \"notes\" in d_jsonld[i]:\n",
    "        c_no +=1\n",
    "        \n",
    "\n",
    "c_ima1 = 0\n",
    "for i in d_json:\n",
    "    if \"image\" in d_json[i]:\n",
    "        c_ima1+=1\n",
    "\n",
    "c_na1 = 0\n",
    "for i in d_json:\n",
    "    if \"nationality\" in d_json[i]:\n",
    "        c_nc_no1 = 0\n",
    "\n",
    "for i in d_json:\n",
    "    if \"notes\" in d_json[i]:\n",
    "        c_no1 +=1a1 +=1\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
